import torch
from torch import nn
from torch.nn import functional as F
from torch.nn.parameter import Parameter
import math
import torch
import numpy as np
from torch import optim
import torch.nn.functional as F
from sklearn.mixture import GaussianMixture
import os
import pyro.distributions as dist
import pyro
import torch.multiprocessing as mp




def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Linear") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)

class Ours:
    # mp.set_start_method('fork') 
    """This is the trainer for the Variational Deep Embedding (VaDEIV).
    """

    def __init__(self, args, device, dataloader_train, dataloader_test):
        self.autoencoder = Autoencoder(feature_dim = args.feature_dim, latent_dim_zt = args.latent_dim_t, 
                                       latent_dim_zc = args.latent_dim,hidden_dim = args.hidden_dim).to(device)
        self.VaDEIV = VaDEIV(n_classes_zc=args.comp_num_zc, 
                             feature_dim = args.feature_dim, latent_dim_zt=args.latent_dim_t,
                             latent_dim_zc=args.latent_dim,treatment_type=args.treatment,hidden_dim = args.hidden_dim,
                             use_dist_net =args.use_dist_net, use_reg_net = args.use_reg_net, use_flex_enc=args.use_flex_enc,
                             use_reconst_x=args.use_reconst_x,hyp_tdist = args.hyp_tdist, hyp_treg= args.hyp_treg, 
                             hyp_yreg=args.hyp_yreg,hyp_mi = args.hyp_mi,device = device).to(device)
        
        self.device = device
        self.args = args
        
        self.dataloader_train = dataloader_train
        self.dataloader_test = dataloader_test
        
        

    def pretrain(self):
        """Here we train an stacked autoencoder which will be used as the initialization for the VaDEIV. 
        This initialization is usefull because reconstruction in VAEs would be weak at the begining
        and the models are likely to get stuck in local minima.
        """
        optimizer = optim.Adam(self.autoencoder.parameters(), lr=0.002)
        self.autoencoder.apply(weights_init_normal) #intializing weights using normal distribution.
        self.autoencoder.train()
        print('Training the autoencoder...')
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(device)
    
        for epoch in range(30):
            total_loss = 0
            for x, _, _ in self.dataloader_train:
                optimizer.zero_grad()
                x = x.to(self.device)
                x= x.to(torch.float32)
                x_hat = self.autoencoder(x)
                loss = F.mse_loss(x_hat, x) # just reconstruction
                loss=loss.to(torch.float32)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print('Training Autoencoder... Epoch: {}, Loss: {}'.format(epoch, total_loss))
        self.train_GMM() #training a GMM for initialize the VaDEIV
        self.save_weights_for_VaDEIV() #saving weights for the VaDEIV


    def train_GMM(self):
        """It is possible to fit a Gaussian Mixture Model (GMM) using the latent space 
        generated by the stacked autoencoder. This way, we generate an initialization for 
        the priors (pi, mu, var) of the VaDEIV model.
        """
        print('Fiting Gaussian Mixture Model...')

        x = torch.cat([data[0] for data in self.dataloader_train]).to(self.device) #all x samples.
        zc = self.autoencoder.encode_zc(x)
        
        #gmm1
        self.gmm1 = GaussianMixture(n_components=self.args.comp_num_zc, covariance_type='diag')
        self.gmm1.fit(zc.cpu().detach().numpy())
        

    def save_weights_for_VaDEIV(self):
        """Saving the pretrained weights for the encoder, decoder, pi, mu, var.
        """
        print('Saving weights.')
        state_dict = self.autoencoder.state_dict()

        self.VaDEIV.load_state_dict(state_dict, strict=False)
        self.VaDEIV.pi_prior_zc.data = torch.from_numpy(self.gmm1.weights_).float().to(self.device)
        self.VaDEIV.mu_prior_zc.data = torch.from_numpy(self.gmm1.means_).float().to(self.device)
        self.VaDEIV.log_var_prior_zc.data = torch.log(torch.from_numpy(self.gmm1.covariances_)).float().to(self.device)
  
        torch.save(self.VaDEIV.state_dict(), self.args.pretrained_path)    

    def train(self):
        """
        """
        if self.args.pretrain==True:
            self.VaDEIV.load_state_dict(torch.load(self.args.pretrained_path,
                                                 map_location=self.device))
                    
        elif self.args.pretrain==False:
            self.VaDEIV.apply(weights_init_normal)
        self.optimizer = optim.Adam(self.VaDEIV.parameters(), lr=self.args.lr)
        lr_scheduler = torch.optim.lr_scheduler.StepLR(
                    self.optimizer, step_size=10, gamma=0.9)
        # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(
        #     self.optimizer, gamma=0.9, last_epoch=-1, verbose=False)
        print('Training VaDEIV...')
        
        ## early stopping ## 
        best_loss = 1e37
        if self.args.highdim:
            early_stopping_epochs = 10
        else:
            early_stopping_epochs=3
        early_stop_counter = 0
        for epoch in range(self.args.num_epochs):
            self.VaDEIV.train()
            total_loss = 0

            for x, t,y in self.dataloader_train:
                self.optimizer.zero_grad()
                x = x.to(self.device)
                x = x.to(torch.float32)
                
                t = t.to(self.device)
                t = t.to(torch.float32)
                
                y = y.to(self.device)
                y = y.to(torch.float32)
                
                loss = self.VaDEIV.compute_loss(x,t,y)
                loss=loss.to(torch.float32)         
                loss.backward()
                self.optimizer.step()
                total_loss += loss.item()
                
            print('Training VaDEIV... Epoch: {}, Loss: {}'.format(epoch, total_loss))
            if total_loss>best_loss:
                early_stop_counter+=1 
            else:
                best_loss = total_loss
                early_stop_counter=0
            
            if early_stop_counter>=early_stopping_epochs:
                print('early stopping!')
                break 
            
            lr_scheduler.step()



class VaDEIV(torch.nn.Module):    
    def __init__(self,n_classes_zc, feature_dim, latent_dim_zt,latent_dim_zc,treatment_type,hidden_dim,
                 use_dist_net, use_reg_net,use_flex_enc,use_reconst_x, hyp_tdist, hyp_treg, hyp_yreg, hyp_mi,device):
        super(VaDEIV, self).__init__()
        
        self.treatment_type = treatment_type
        self.hidden_dim = hidden_dim
        self.use_dist_net = use_dist_net
        self.use_reg_net = use_reg_net
        self.use_flex_enc = use_flex_enc
        self.use_reconst_x = use_reconst_x
        self.hyp_tdist, self.hyp_treg, self.hyp_yreg, self.hyp_mi= hyp_tdist, hyp_treg, hyp_yreg, hyp_mi
        self.device = device 
        self.latent_dim_zt = latent_dim_zt
        self.latent_dim_zc = latent_dim_zc
        
        if self.use_flex_enc:
            self.pi_prior_zc = Parameter(torch.ones(n_classes_zc)/n_classes_zc)
            self.mu_prior_zc = Parameter(torch.zeros(n_classes_zc, latent_dim_zc))
            self.log_var_prior_zc = Parameter(torch.randn(n_classes_zc, latent_dim_zc))
        
        
        ## Encoder for zc q(zc|d)
        self.enc_zc1 = nn.Linear(feature_dim, self.hidden_dim) 
        self.enc_zc2 = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.enc_zc3 = nn.Linear(self.hidden_dim, self.hidden_dim) 
        self.mu_zc = nn.Linear(self.hidden_dim, latent_dim_zc) #Latent mu
        self.log_var_zc = nn.Linear(self.hidden_dim, latent_dim_zc) #Latent logvar
        
        ## Encoder for q(zt|d) #pretrain here
        self.enc_zt1 = nn.Linear(feature_dim,self.hidden_dim) 
        self.enc_zt2 = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.enc_zt3 = nn.Linear(self.hidden_dim, self.hidden_dim) 
        self.mu_zt = nn.Linear(self.hidden_dim, latent_dim_zt) #Latent mu
        self.log_var_zt = nn.Linear(self.hidden_dim, latent_dim_zt) #Latent logvar
        
        ## prior p(zt|D)
        self.prior_zt1 = nn.Linear(feature_dim,self.hidden_dim) 
        self.prior_zt2 = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.prior_zt3 = nn.Linear(self.hidden_dim, self.hidden_dim)         
        self.prior_mu_zt = nn.Linear(self.hidden_dim, latent_dim_zt) #Latent mu
        self.prior_log_var_zt = nn.Linear(self.hidden_dim, latent_dim_zt) #Latent logvar
        
        ## Decoder p(d|zc,zt)
        self.reconx1= nn.Linear((latent_dim_zc+latent_dim_zt),self.hidden_dim) 
        self.reconx2 = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.reconx3 = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.mu_x = nn.Linear(self.hidden_dim, feature_dim) 
        
        ### treatment dist.  p(t|zc) ==p(x|z) ### CE ID 
        if self.use_dist_net:
            self.treatment_logit = nn.Linear(latent_dim_zt,1)  
            
            # b: continuous -> MS
            self.cont_dist = nn.Linear((latent_dim_zt),self.hidden_dim)
            self.treatment_mu = nn.Linear(self.hidden_dim, 1)
            self.treatment_logvar = nn.Linear(self.hidden_dim, 1)
        
        if self.use_reg_net:
             #Treatment net f(t|zc,zt)
            self.treg1 = nn.Linear((latent_dim_zc+latent_dim_zt),self.hidden_dim)
            self.treg2 = nn.Linear(self.hidden_dim, 1)
            ## t: binary
            #Outcome Net f(y|t,zt) == f(y|x,c)
            self.yreg_t1_1 = nn.Linear((latent_dim_zc), self.hidden_dim) 
            self.yreg_t1_2 = nn.Linear(self.hidden_dim, self.hidden_dim)
            self.y_mu_1= nn.Linear(self.hidden_dim, 1) 
    
            #t=0 Net
            self.yreg_t0_1 = nn.Linear((latent_dim_zc), self.hidden_dim) 
            self.yreg_t0_2 = nn.Linear(self.hidden_dim, self.hidden_dim)
            
            ## t=continuous 
            self.yreg_con1 = nn.Linear((latent_dim_zc+1), self.hidden_dim) 
            self.y_mu_0 = nn.Linear(self.hidden_dim, 1) 
        

    def encode_zc(self, x):
        x= x.to(torch.float32)
        h = F.relu(self.enc_zc1(x))
        h = F.relu(self.enc_zc2(h))
        h = F.relu(self.enc_zc3(h))
        return self.mu_zc(h)+1e-6, self.log_var_zc(h).clamp(min=-10, max=10)
    
    def encode_zt(self,x):
        x= x.to(torch.float32)
        x = x+1e-6 
        h = F.relu(self.enc_zt1(x))
        # h = F.relu(self.enc_zt2(h))
        h = F.relu(self.enc_zt3(h))
        return self.mu_zt(h), self.log_var_zt(h).clamp(min=-1, max=5)
    
    def prior_zt(self, x):
        x= x.to(torch.float32)
        h = F.relu(self.prior_zt1(x))
        h = F.relu(self.prior_zt2(h))
        h = F.relu(self.prior_zt3(h))
        return self.prior_mu_zt(h), self.prior_log_var_zt(h).clamp(min=-1, max=5)

    def decode(self, zc,zt):
        zc, zt = zc.to(torch.float32),zt.to(torch.float32)
        z = torch.cat((zc,zt),-1)
        h = F.relu(self.reconx1(z))
        h = F.relu(self.reconx2(h))
        h = F.relu(self.reconx3(h))
        return self.mu_x(h)
    
    
    def treatment_net(self, zc,zt):
        zc, zt = zc.to(torch.float32),zt.to(torch.float32)
        z = torch.cat((zc,zt),-1)
        h = F.relu(self.treg1(z))
        
        ### Treatment: Binary ###
        if self.treatment_type=='b':
            h = F.relu(self.treg2(h))
            tvector = torch.nn.Sigmoid()(h)
            return tvector
        
        elif self.treatment_type=='con':
             return self.treg2(h)
    
    def treatment(self, zt):
        zt = zt.to(torch.float32)
        ### Treatment: Binary ###
        if self.treatment_type == 'b':
            logit = self.treatment_logit(zt)
            return logit
        elif self.treatment_type == 'con':
            h = F.relu(self.cont_dist(zt))
            return (self.treatment_mu(h), self.treatment_logvar(h).clamp(min=-10, max=10))
       
        
    
    def outcome(self, t,zc,):
        '''
        t: [1,0,0,1,...,]
        zt: [[1,2,...,1],[391,23,4,..,1],...,[1,2,32,1,...,1]]
        output: (t=1 net mu, sigma), (t=0 net mu, sigma)
        '''
        t, zc = t.to(torch.float32),zc.to(torch.float32)
        c = zc[:,:]
        
        if self.treatment_type == 'b':
            t1_index = (t==1).nonzero(as_tuple=True)[0]
            t0_index = (t==0).nonzero(as_tuple=True)[0]
              
            # t=1 net
            
            c1 = c[t1_index]
            c0 = c[t0_index]        
            # t=1 net
            h1 = F.relu(self.yreg_t1_1(c1))       
            h1 = F.relu(self.yreg_t1_2(h1))
            y_mu1 = self.y_mu_1(h1)
            
            #t=0 Net 
            h0 = F.relu(self.yreg_t0_1(c0))
            h0 = F.relu(self.yreg_t0_2(h0))
            y_mu0= self.y_mu_0(h0)
            
            return (y_mu1, y_mu0)
        
        elif self.treatment_type=='con':
            # t = t.unsqueeze(1)
            inp = torch.cat((c,t),-1)
            h1 = F.relu(self.yreg_con1(inp))       
            h1 = F.relu(self.yreg_t1_2(h1))
            y_mu = self.y_mu_1(h1)
            
            return y_mu
        
    def analysis(self, x):
        x = x.to(torch.float32).requires_grad_(True)
        x = x + 1e-6
        # Forward pass
        h = F.relu(self.enc_zt1(x))
        h = F.relu(self.enc_zt3(h))
        mu_zt = self.mu_zt(h)
        lst = []
        for i in range(mu_zt.shape[0]):
            vector = []
            for j in range(mu_zt.shape[1]):
                tmp = torch.autograd.grad(mu_zt[i,j], x, retain_graph=True)[0].cpu().detach().numpy()[i]
                vector.append(tmp)
            lst.append(abs(np.array(vector)).mean(axis=0))
        return np.array(lst)

    def reparameterize(self, mu, log_var):
        std = torch.exp(log_var/2)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x,t,y):
        '''
        x = [batch_size, feature_dim]
        t = [batch_size] 
        y = [batch_size]
        
        output: samples, parameters
        '''
        
        mu_zc, log_var_zc = self.encode_zc(x)
        mu_zt, log_var_zt = self.encode_zt(x)
        
        prior_mu_zt, prior_log_var_zt = self.prior_zt(x)
        
        # ec = self.encode_ec(x,mu_zt)
        zc = self.reparameterize(mu_zc, log_var_zc)
        zt = self.reparameterize(mu_zt, log_var_zt)
        # zt_prior = self.reparameterize(prior_mu_zt,prior_log_var_zt)
        
        # reconstruct x (d)
        x_hat = self.decode(zc,zt)
        
        # auxiliary loss t|zc,zt , y|t,zt 
        if self.use_dist_net and self.use_reg_net:
            if self.treatment_type == 'b':
                logit_t = self.treatment(zt) # p(t|zc)
                t_sample = dist.Bernoulli(logits=logit_t).sample()
                t_rep = self.treatment_net(zc,zt)# f(t|zc,zt)
                
                # y|zt,t=observed
                if self.use_reconst_x:
                    y1_mu, y0_mu = self.outcome(t_sample,zc)
                else:                    
                    y1_mu, y0_mu = self.outcome(t.unsqueeze(1),zc)
                y_sample =(y1_mu, y0_mu)
            
            elif self.treatment_type =='con':
                logit_t, log_var_t = self.treatment(zt) # p(t|zc) 
                t_rep = self.treatment_net(zc,zt)# f(t|zc,zt)
                t_sample = dist.Normal(logit_t,torch.exp(log_var_t/2)).sample()
                if self.use_reconst_x:
                    y_sample = self.outcome(t_sample,zc) # f(y|t,zt)
                else:
                    y_sample = self.outcome(t.unsqueeze(1),zc)
                            
            return x_hat, mu_zc, mu_zt, prior_mu_zt, log_var_zc, log_var_zt, prior_log_var_zt, zc,zt,logit_t, y_sample, t_rep,t_sample
        
        else:        
            return x_hat, mu_zc, mu_zt, prior_mu_zt, log_var_zc, log_var_zt, prior_log_var_zt, zc,zt
    
    
    def compute_loss(self,x,t,y):
        if self.use_dist_net and self.use_reg_net:
            x_hat, mu_zc, mu_zt, prior_mu_zt, log_var_zc, log_var_zt, prior_log_var_zt, zc,zt, logit_t, y_sample, t_rep,t_sample = self.forward(x,t,y)        
        else:
            x_hat, mu_zc, mu_zt, prior_mu_zt, log_var_zc, log_var_zt, prior_log_var_zt, zc,zt = self.forward(x,t,y)
       
        # Reconstruction Loss p(x|zc,zt) == p(d|z,c)
        log_p_x_given_z = F.mse_loss(x_hat, x,reduction='sum') 
       
        # zt loss (loss related to confounding rep C)= KL[q(C|D)||P(C|D)]
        kl_zt = self.kld(mu_zt,log_var_zt,prior_mu_zt,prior_log_var_zt)
        
        loss = log_p_x_given_z +kl_zt
        
        # zc loss 
        if self.use_flex_enc:
            p_ez_zc = self.pi_prior_zc # p(ez)
            gamma_zc = self.compute_gamma_zc(zc, p_ez_zc)

            h_zc = log_var_zc.exp().unsqueeze(1) + (mu_zc.unsqueeze(1) - self.mu_prior_zc).pow(2)
            h_zc = torch.sum(self.log_var_prior_zc + h_zc / self.log_var_prior_zc.exp(), dim=2)
            log_p_zc_given_c = 0.5 * torch.sum(gamma_zc * h_zc) # log p(zc|ez)
            log_p_ez_zc = torch.sum(gamma_zc * torch.log(p_ez_zc + 1e-9)) # log p(ez)
            log_q_ez_zc_given_x = torch.sum(gamma_zc * torch.log(gamma_zc + 1e-9)) # log q(ez,zc|d)
            log_q_zc_given_x = 0.5 * torch.sum(1 + log_var_zc) # log q(zc|d)
            
            zc_loss = (log_p_zc_given_c - log_p_ez_zc +  log_q_ez_zc_given_x - log_q_zc_given_x)
            
        else: 
            zc_loss = torch.sum(-0.5 * torch.sum(1 + log_var_zc - mu_zc ** 2 - log_var_zc.exp(), dim = 1), dim = 0)
            
        loss += zc_loss
        
        corr = torch.corrcoef(torch.cat((zc.T,zt.T),0))      
        mi_loss_zc_zt=torch.sum(corr[:self.latent_dim_zc,self.latent_dim_zc:]**2)
        
        
        # t,y loss (Treatment, Outcome Loss)
        if self.use_dist_net and self.use_reg_net:
            if self.treatment_type =='b':
                t1_index = (t_sample==1).nonzero(as_tuple=True)[0]
                t0_index = (t_sample==0).nonzero(as_tuple=True)[0]
                t1_index_real = (t==1).nonzero(as_tuple=True)[0]
                t0_index_real = (t==0).nonzero(as_tuple=True)[0]
                
                logit_t = 1/(1+torch.exp(-logit_t))
                log_q_t_given_z = -torch.sum(torch.log(logit_t+1e-9)[t1_index_real])-torch.sum(torch.log(1-logit_t+1e-9)[t0_index_real])
                t_mse = F.binary_cross_entropy(t_rep.squeeze(),t,reduction='sum')
                
                

                if self.use_reconst_x:
                    y1 = y[t1_index]
                    y0 = y[t0_index]
                else:
                    y1 = y[t1_index_real]
                    y0 = y[t0_index_real]
                y1_mu = y_sample[0]
                y0_mu = y_sample[1]
                y_mse= F.mse_loss(y1_mu.squeeze(),y1,reduction='sum')+F.mse_loss(y0_mu.squeeze(),y0,reduction='sum')        
        
            elif self.treatment_type =='con':
                log_q_t_given_z = F.mse_loss(logit_t.squeeze(),t,reduction='sum')
                t_mse = F.mse_loss(t_rep.squeeze(),t,reduction='sum')
                y_mse = F.mse_loss(y_sample.squeeze(),y,reduction='sum')
            
            loss += self.hyp_tdist*log_q_t_given_z+ self.hyp_treg*t_mse + self.hyp_yreg*y_mse
            
        # Final Loss  
        loss /= x.size(0)
        loss +=self.hyp_mi*mi_loss_zc_zt
        
        return loss

    def kld(self,mu1,logvar1,mu2,logvar2):
        '''
        kl divergence loss
        https://github.com/lttsh/CVAE/blob/master/CVAE/cvae_trainer.py#L9
        '''
        return torch.sum(0.5 * (torch.sum(logvar2-logvar1, axis=-1) - (logvar2.shape[1]+0.0)\
        + torch.sum(torch.exp(logvar1-logvar2), axis=-1) + torch.sum((mu1-mu2)**2/torch.exp(logvar2), axis=-1)))
        
    def compute_gamma_zc(self, z, p_c):
        h = (z.unsqueeze(1) - self.mu_prior_zc).pow(2) / self.log_var_prior_zc.exp()
        h += self.log_var_prior_zc
        h += torch.Tensor([np.log(np.pi*2)]).to(self.device)
        p_z_c = torch.exp(torch.log(p_c + 1e-9).unsqueeze(0) - 0.5 * torch.sum(h, dim=2)) + 1e-9
        gamma = p_z_c / torch.sum(p_z_c, dim=1, keepdim=True)
        return gamma
        
        
class Autoencoder(torch.nn.Module):
    def __init__(self, feature_dim, latent_dim_zt,latent_dim_zc,hidden_dim):
        super(Autoencoder, self).__init__()
        
        self.hidden_dim = hidden_dim
        
        self.enc_zc1 = nn.Linear(feature_dim, self.hidden_dim) #Encoder
        self.enc_zc2 = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.enc_zc3 = nn.Linear(self.hidden_dim, self.hidden_dim) 

        self.enc_zt1 = nn.Linear(feature_dim, self.hidden_dim) #Encoder
        self.enc_zt2 = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.enc_zt3 = nn.Linear(self.hidden_dim, self.hidden_dim) 


        self.mu_zc = nn.Linear(self.hidden_dim, latent_dim_zc) #Latent code
        self.mu_zt = nn.Linear(self.hidden_dim, latent_dim_zt) #Latent code

        self.reconx1= nn.Linear((latent_dim_zc+latent_dim_zt), self.hidden_dim) 
        self.reconx2 = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.reconx3 = nn.Linear(self.hidden_dim, self.hidden_dim)
        self.mu_x = nn.Linear(self.hidden_dim, feature_dim) #Latent mu

    def encode_zc(self, x):
        x= x.to(torch.float32)
        h = F.relu(self.enc_zc1(x))
        h = F.relu(self.enc_zc2(h))
        h = F.relu(self.enc_zc3(h))
        return self.mu_zc(h)
    
    def encode_zt(self,x):
        x= x.to(torch.float32)
        h = F.relu(self.enc_zt1(x))
        h = F.relu(self.enc_zt2(h))
        h = F.relu(self.enc_zt3(h))
        return self.mu_zt(h)
        
    
    def decode(self, zc,zt):
        zc,zt= zc.to(torch.float32),zt.to(torch.float32)
        z = torch.cat((zc,zt),-1)
        h = F.relu(self.reconx1(z))
        h = F.relu(self.reconx2(h))
        h = F.relu(self.reconx3(h))
        return self.mu_x(h)
    
    def forward(self, x):
        x= x.to(torch.float32)
        
        zc = self.encode_zc(x)
        zt = self.encode_zt(x)
        
        x_hat = self.decode(zc,zt)
        return x_hat
